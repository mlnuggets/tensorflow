{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## How to build TensorFlow models with the Keras functional API: examples, code, and notebooks\n\nClick the image below to read the post online.\n\n<a target=\"_blank\" href=\"https://www.machinelearningnuggets.com/tensorflow-keras-functional-api\n\"><img src=\"https://www.machinelearningnuggets.com/ezoimgfmt/digitalpress.fra1.cdn.digitaloceanspaces.com/mhujhsj/2022/07/logho-1.png?ezimgfmt=ng:webp/ngcb1\" alt=\"Open in ML Nuggets\"></a>","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow\ntensorflow.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras \nfrom tensorflow.keras import layers\nparameters = {\"shape\":28, \"activation\": \"relu\", \"classes\": 10, \"units\":12, \"optimizer\":\"adam\", \"epochs\":1,\"kernel_size\":3,\"pool_size\":2, \"dropout\":0.5}\n# Setup the layers\nmodel = keras.Sequential(\n  [\n      layers.Conv2D(32, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), input_shape =(parameters[\"shape\"], parameters[\"shape\"], 1),activation=parameters[\"activation\"]),\n      layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"])),\n      layers.Conv2D(64, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), activation=parameters[\"activation\"]),\n      layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"])),\n      layers.Flatten(),\n      layers.Dropout(parameters[\"dropout\"]),\n      layers.Dense(parameters[\"classes\"], activation=\"softmax\"),\n  ]\n)","metadata":{"id":"HpOPYQZEeWrY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"j8s_6DLmc0Jb","outputId":"f802325a-88f6-402c-e46d-f3c8cb7934a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, \"model.png\")\n","metadata":{"id":"KZjxQUEFc8e7","outputId":"8c529ede-51aa-4cc8-b50d-8aa4e8ea0e14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)\n","metadata":{"id":"uVkyrG-AdCBx","outputId":"9b96fe04-1d5b-4bb6-c589-861895daf92b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  inputs = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 1))\n","metadata":{"id":"MPmy05zIeoiS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.shape","metadata":{"id":"QT9f451Ie06w","outputId":"1c4c0e09-4b13-451e-8123-38ccd3cbb10b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.dtype\n","metadata":{"id":"kNTw6Z3vfjth","outputId":"3437dd1f-02af-45a7-9268-ea3559728d90","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv2D = layers.Conv2D(32, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), input_shape =(parameters[\"shape\"], parameters[\"shape\"], 1),activation=parameters[\"activation\"])\nx = conv2D(inputs)","metadata":{"id":"myw5_XpWhJuh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = {\"shape\":28, \"activation\": \"relu\", \"classes\": 10, \"units\":12, \"optimizer\":\"adam\", \"epochs\":1,\"kernel_size\":3,\"pool_size\":2, \"dropout\":0.5}\ninputs = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 1))\nconv2D = layers.Conv2D(32, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), input_shape =(parameters[\"shape\"], parameters[\"shape\"], 1),activation=parameters[\"activation\"])(inputs)\nmaxPooling2D = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D)\nconv2D_2 =layers.Conv2D(64, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), activation=parameters[\"activation\"])(maxPooling2D)\nmaxPooling2D_2 = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D_2)\nflatten =   layers.Flatten()(maxPooling2D_2)\ndropout = layers.Dropout(parameters[\"dropout\"])(flatten)\noutputs = layers.Dense(parameters[\"classes\"], activation=\"softmax\")(dropout)\n","metadata":{"id":"Ltbi1nwyjqF0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n","metadata":{"id":"mD3JXrhZeltU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, \"model.png\",show_shapes=True)\n","metadata":{"id":"MiKqhZgD-7id","outputId":"e7bd0fe1-e0a8-4558-d21e-174b245b5940","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and evaluation of Functional API models","metadata":{"id":"_OhQX7KG_jye"}},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.RMSprop(),\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n\ntest_scores = model.evaluate(x_test, y_test, verbose=2)\nprint(\"Test loss:\", test_scores[0])\nprint(\"Test accuracy:\", test_scores[1])","metadata":{"id":"g1fTHDx2_iMr","outputId":"39e7702d-5597-4019-9280-da477e243290","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"saved_model\")\ndel model\nmodel = keras.models.load_model(\"saved_model\")\nmodel.summary()","metadata":{"id":"LzBNsDe2BuNZ","outputId":"b2d0dae9-3153-414c-c3c2-0b0fd27b7999","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary(expand_nested=True)","metadata":{"id":"v346Ca_DCR0k","outputId":"b530595d-868e-44d8-d2a5-adbdfb22f96d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to convert a Functional model to a Sequential API model","metadata":{"id":"c_EEMpGpGnEa"}},{"cell_type":"code","source":"seq_model = keras.models.Sequential()\nfor layer in model.layers:\n    seq_model.add(layer)\nseq_model.summary()","metadata":{"id":"FLjOcxVxGs22","outputId":"b7c0c767-ade5-41c9-e18b-4aef104af833","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Sequential model to Functional model","metadata":{"id":"h9u42oztCtTA"}},{"cell_type":"code","source":"inputs = keras.Input(batch_shape=seq_model.layers[0].input_shape)\nx = inputs\nfor layer in seq_model.layers:\n    x = layer(x) \noutputs = x\nfunc_model = keras.Model(inputs=inputs, outputs=outputs, name=\"func_mnist_model\")","metadata":{"id":"DSPvNIJOMIIw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"func_model.summary()","metadata":{"id":"Xb44stznFjmL","outputId":"686b8051-f788-4f23-fe90-8500fb54ab9f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standard network models","metadata":{"id":"V2xM9d0eY0Nu"}},{"cell_type":"markdown","source":"### Multilayer perceptron","metadata":{"id":"1Tl-J5ONY27i"}},{"cell_type":"code","source":"inputs = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 1))\ndense1 = layers.Dense(128)(inputs)\ndropout = layers.Dropout(parameters[\"dropout\"])(dense1)\ndense2 = layers.Dense(128)(dropout)\ndropout1 = layers.Dropout(parameters[\"dropout\"])(dense2)\noutputs = layers.Dense(parameters[\"classes\"], activation=\"softmax\")(dropout1)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)\n","metadata":{"id":"E7jh3CPbYzkv","outputId":"938fae82-41d2-46c2-bdf3-fec7d255dfee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolutional Neural Network","metadata":{"id":"wMELVyGuY6D2"}},{"cell_type":"code","source":"inputs = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 1))\nconv2D = layers.Conv2D(32, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), input_shape =(parameters[\"shape\"], parameters[\"shape\"], 1),activation=parameters[\"activation\"])(inputs)\nmaxPooling2D = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D)\nconv2D_2 =layers.Conv2D(64, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), activation=parameters[\"activation\"])(maxPooling2D)\nmaxPooling2D_2 = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D_2)\nflatten =   layers.Flatten()(maxPooling2D_2)\ndropout = layers.Dropout(parameters[\"dropout\"])(flatten)\noutputs = layers.Dense(parameters[\"classes\"], activation=\"softmax\")(dropout)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)\n\n","metadata":{"id":"ao1WisTuY_pW","outputId":"97b0cc67-6cb4-406e-ffe9-22b5e58bbac9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recurrent Neural Network","metadata":{"id":"bxrUSpEIZAFA"}},{"cell_type":"code","source":"inputs = keras.Input(784,)\nembedding = layers.Embedding(512, 64, input_length=1024)(inputs)\nbidirectional1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(embedding)\nbidirectional2 = layers.Bidirectional(layers.LSTM(64,))(bidirectional1)\ndense1 = layers.Dense(32, activation='relu')(bidirectional2)\noutputs = layers.Dense(1, activation='sigmoid')(dense1)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"lstm_model\")\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"pSzg2LN0bc5b","outputId":"08ad1aa4-8bfc-4853-8b81-5e15122e56e2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shared input layer","metadata":{"id":"aWxl4gp2fP0P"}},{"cell_type":"code","source":"inputs = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 1))\nconv2D = layers.Conv2D(32, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), input_shape =(parameters[\"shape\"], parameters[\"shape\"], 1),activation=parameters[\"activation\"])(inputs)\nmaxPooling2D = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D)\nflatten1 =   layers.Flatten()(maxPooling2D)\n\nconv2D_2 = layers.Conv2D(64, kernel_size=(parameters[\"kernel_size\"], parameters[\"kernel_size\"]), activation=parameters[\"activation\"])(inputs)\nmaxPooling2D_2 = layers.MaxPooling2D(pool_size=(parameters[\"pool_size\"], parameters[\"pool_size\"]))(conv2D_2)\nflatten2 =   layers.Flatten()(maxPooling2D_2)\n\n# merge layers\nmerged_layers = layers.concatenate([flatten1, flatten2])\n\ndropout = layers.Dropout(parameters[\"dropout\"])(merged_layers)\noutputs = layers.Dense(parameters[\"classes\"], activation=\"softmax\")(dropout)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"AYSmuIwYfPfT","outputId":"5e2e110d-65f7-407f-c1e5-83ba28e270db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shared feature extraction layer\n","metadata":{"id":"CJGZac91hDjS"}},{"cell_type":"code","source":"inputs = keras.Input(784,)\nembedding = layers.Embedding(512, 64, input_length=1024)(inputs)\nbidirectional1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(embedding)\nbidirectional2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(embedding)\n\n# merge layers\nmerged_layers = layers.concatenate([bidirectional1, bidirectional2])\n\ndense1 = layers.Dense(32, activation='relu')(merged_layers)\noutputs = layers.Dense(1, activation='sigmoid')(dense1)\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=\"lstm_model\")\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"0VALqNvchDQx","outputId":"eb420831-171f-4681-f664-f0cf3d4bd62e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multiple input model","metadata":{"id":"Cig3jxFhoMFA"}},{"cell_type":"code","source":"input1 = keras.Input(shape=(16,))\nx1 =layers.Dense(8, activation='relu')(input1)\ninput2 = layers.Input(shape=(32,))\nx2 = layers.Dense(8, activation='relu')(input2)\n# equivalent to `added = tf.keras.layers.add([x1, x2])`\nadded = layers.Add()([x1, x2])\nout = layers.Dense(4)(added)\nmodel = keras.Model(inputs=[input1, input2], outputs=out)\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"LBr6Ba3HoOs7","outputId":"b90ade22-ae02-4140-9775-3794541d09ae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multiple output model","metadata":{"id":"aqWHiVjpvZPb"}},{"cell_type":"code","source":"image_input = keras.Input(shape=(parameters[\"shape\"], parameters[\"shape\"], 3), name=\"images\") \nx = layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu')(image_input)\nx = layers.MaxPooling2D(pool_size=(2,2))(x)\n\nx = layers.Conv2D(filters=32,kernel_size=(3,3), activation='relu')(x)\nx = layers.Dropout(0.25)(x)\nx = layers.Conv2D(filters=64,kernel_size=(3,3), activation='relu')(x)\nx = layers.MaxPooling2D(pool_size=(2,2))(x)\n\nx = layers.Dropout(0.25)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.25)(x)\n\ngender_prediction = layers.Dense(3, activation='softmax')(x)\nage_prediction = layers.Dense(3, activation='softmax')(x)\n\nmodel = keras.Model(\n    inputs=image_input,\n    outputs=[gender_prediction, age_prediction],\n)\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"pnfAvpg7vYbK","outputId":"25460df4-ab8d-4651-d222-92dc95d223b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use the same graph of layers to define multiple models","metadata":{"id":"cua_A8RYdW3G"}},{"cell_type":"code","source":"encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\nx = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\nx = layers.Conv2D(32, 3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(3)(x)\nx = layers.Conv2D(32, 3, activation=\"relu\")(x)\nx = layers.Conv2D(16, 3, activation=\"relu\")(x)\nencoder_output = layers.GlobalMaxPooling2D()(x)\n\nencoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n# encoder.summary()\n\nx = layers.Reshape((4, 4, 1))(encoder_output)\nx = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\nx = layers.UpSampling2D(3)(x)\nx = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\ndecoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n\nautoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n# autoencoder.summary()\nkeras.utils.plot_model(autoencoder, \"autoencoder.png\",show_shapes=True)","metadata":{"id":"tMxgnC26dLmq","outputId":"60c1d8fe-0bf3-4f30-8899-980669bedfaa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(autoencoder, \"autoencoder.png\",show_shapes=True)\n","metadata":{"id":"uuNth52FdZfy","outputId":"baee7cf5-9ae0-4128-8889-61aa25f3a06a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## All models are callable, just like layers","metadata":{"id":"OtL6hpuvdnN_"}},{"cell_type":"code","source":"encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\nx = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\nx = layers.Conv2D(32, 3, activation=\"relu\")(x)\nx = layers.MaxPooling2D(3)(x)\nx = layers.Conv2D(32, 3, activation=\"relu\")(x)\nx = layers.Conv2D(16, 3, activation=\"relu\")(x)\nencoder_output = layers.GlobalMaxPooling2D()(x)\n\nencoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\nencoder.summary()\n\ndecoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\nx = layers.Reshape((4, 4, 1))(decoder_input)\nx = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\nx = layers.UpSampling2D(3)(x)\nx = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\ndecoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n\ndecoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\ndecoder.summary()\n\nautoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\nencoded_img = encoder(autoencoder_input)\ndecoded_img = decoder(encoded_img)\nautoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\nautoencoder.summary()","metadata":{"id":"JovBX25ydgM3","outputId":"cd7c5670-3f29-42d0-98b4-80c7d45f8b03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to ensemble models","metadata":{"id":"mdzlLJ9fd2m4"}},{"cell_type":"code","source":"def get_model():\n    inputs = keras.Input(shape=(128,))\n    outputs = layers.Dense(1)(inputs)\n    return keras.Model(inputs, outputs)\n\n\nmodel1 = get_model()\nmodel2 = get_model()\nmodel3 = get_model()\n\ninputs = keras.Input(shape=(128,))\ny1 = model1(inputs)\ny2 = model2(inputs)\ny3 = model3(inputs)\noutputs = layers.average([y1, y2, y3])\nensemble_model = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"id":"jzI3ZOLTdp5_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models with multiple inputs and outputs","metadata":{"id":"YiiyJUQLd5Rr"}},{"cell_type":"code","source":"num_tags = 12  # Number of unique issue tags\nnum_words = 10000  # Size of vocabulary obtained when preprocessing text data\nnum_departments = 4  # Number of departments for predictions\n\ntitle_input = keras.Input(\n    shape=(None,), name=\"title\"\n)  # Variable-length sequence of ints\nbody_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\ntags_input = keras.Input(\n    shape=(num_tags,), name=\"tags\"\n)  # Binary vectors of size `num_tags`\n\n# Embed each word in the title into a 64-dimensional vector\ntitle_features = layers.Embedding(num_words, 64)(title_input)\n# Embed each word in the text into a 64-dimensional vector\nbody_features = layers.Embedding(num_words, 64)(body_input)\n\n# Reduce sequence of embedded words in the title into a single 128-dimensional vector\ntitle_features = layers.LSTM(128)(title_features)\n# Reduce sequence of embedded words in the body into a single 32-dimensional vector\nbody_features = layers.LSTM(32)(body_features)\n\n# Merge all available features into a single large vector via concatenation\nx = layers.concatenate([title_features, body_features, tags_input])\n\n# Stick a logistic regression for priority prediction on top of the features\npriority_pred = layers.Dense(1, name=\"priority\")(x)\n# Stick a department classifier on top of the features\ndepartment_pred = layers.Dense(num_departments, name=\"department\")(x)\n\n# Instantiate an end-to-end model predicting both priority and department\nmodel = keras.Model(\n    inputs=[title_input, body_input, tags_input],\n    outputs=[priority_pred, department_pred],\n)","metadata":{"id":"UNvoo3Cldy7r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n","metadata":{"id":"-AaC2yKdd8GJ","outputId":"9e9e3b61-3805-4fa5-be5b-c35b32ec4b18","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile model with multiple inputs","metadata":{"id":"l_M8eq_5eEdk"}},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.RMSprop(1e-3),\n    loss=[\n        keras.losses.BinaryCrossentropy(from_logits=True),\n        keras.losses.CategoricalCrossentropy(from_logits=True),\n    ],\n    loss_weights=[1.0, 0.2],\n)","metadata":{"id":"HCfjJeb7d_rt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.RMSprop(1e-3),\n    loss={\n        \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),\n        \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),\n    },\n    loss_weights={\"priority\": 1.0, \"department\": 0.2},\n)","metadata":{"id":"A-EYmnwOeJ0F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training models with multiple inputs","metadata":{"id":"4syO6Es8eQFD"}},{"cell_type":"code","source":"# Dummy input data\nimport numpy as np\ntitle_data = np.random.randint(num_words, size=(1280, 10))\nbody_data = np.random.randint(num_words, size=(1280, 100))\ntags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")\n\n# Dummy target data\npriority_targets = np.random.random(size=(1280, 1))\ndept_targets = np.random.randint(2, size=(1280, num_departments))\n\nmodel.fit(\n    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n    {\"priority\": priority_targets, \"department\": dept_targets},\n    epochs=2,\n    batch_size=32,\n)","metadata":{"id":"gH7uWY3ReMsQ","outputId":"3b0227ad-e4b2-4f7d-cf27-9c69d02b31aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A toy ResNet model","metadata":{"id":"8jZUu8XIeaUg"}},{"cell_type":"code","source":"inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\nx = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\")(x)\nblock_1_output = layers.MaxPooling2D(3)(x)\n\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nblock_2_output = layers.add([x, block_1_output])\n\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\nx = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\nblock_3_output = layers.add([x, block_2_output])\n\nx = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(10)(x)\n\nmodel = keras.Model(inputs, outputs, name=\"toy_resnet\")\nmodel.summary()","metadata":{"id":"Wb2NYVAreUum","outputId":"b3920710-fcef-4da3-bb88-f5deb540edff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True)","metadata":{"id":"NjfYQMP0ecbK","outputId":"ea86d623-74ed-4d1a-9fec-c9056b81f7af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\nx_train = x_train.astype(\"float32\") / 255.0\nx_test = x_test.astype(\"float32\") / 255.0\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(1e-3),\n    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[\"acc\"],\n)\n# We restrict the data to the first 1000 samples so as to limit execution time\n# on Colab. Try to train on the entire dataset until convergence!\nmodel.fit(x_train[:1000], y_train[:1000], batch_size=64, epochs=1, validation_split=0.2)","metadata":{"id":"W_3eLmFVeejA","outputId":"81fcfaeb-27d5-4941-d0ad-53959f113aa3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Shared layers","metadata":{"id":"iUsaIeroemUO"}},{"cell_type":"code","source":"# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding = layers.Embedding(1000, 128)\n\n# Variable-length sequence of integers\ntext_input_a = keras.Input(shape=(None,), dtype=\"int32\")\n\n# Variable-length sequence of integers\ntext_input_b = keras.Input(shape=(None,), dtype=\"int32\")\n\n# Reuse the same layer to encode both inputs\nencoded_input_a = shared_embedding(text_input_a)\nencoded_input_b = shared_embedding(text_input_b)","metadata":{"id":"KZjV-l54ehyG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract and reuse nodes in the graph of layers","metadata":{"id":"dYUnNwIYerA_"}},{"cell_type":"code","source":"vgg19 = keras.applications.VGG19()","metadata":{"id":"AKaGB_FAeooH","outputId":"8229f370-2379-4351-f275-1fdb16e34ac8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_list = [layer.output for layer in vgg19.layers]","metadata":{"id":"t4NizxKmetmV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n\nimg = np.random.random((1, 224, 224, 3)).astype(\"float32\")\nextracted_features = feat_extraction_model(img)","metadata":{"id":"995-qY19exGX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extend the API using custom layers","metadata":{"id":"CBPvlyKre1IX"}},{"cell_type":"code","source":"import tensorflow as tf\nclass CustomDense(layers.Layer):\n    def __init__(self, units=32):\n        super(CustomDense, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"random_normal\",\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\n\ninputs = keras.Input((4,))\noutputs = CustomDense(10)(inputs)\n\nmodel = keras.Model(inputs, outputs)","metadata":{"id":"Cu4N9_izey3F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDense(layers.Layer):\n    def __init__(self, units=32):\n        super(CustomDense, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"random_normal\",\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\n    def get_config(self):\n        return {\"units\": self.units}\n\n\ninputs = keras.Input((4,))\noutputs = CustomDense(10)(inputs)\n\nmodel = keras.Model(inputs, outputs)\nconfig = model.get_config()\n\nnew_model = keras.Model.from_config(config, custom_objects={\"CustomDense\": CustomDense})","metadata":{"id":"MI0mI0Eke3vj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mix-and-match API styles","metadata":{"id":"b-e3PYyMfJNw"}},{"cell_type":"code","source":"units = 32\ntimesteps = 10\ninput_dim = 5\n\n# Define a Functional model\ninputs = keras.Input((None, units))\nx = layers.GlobalAveragePooling1D()(inputs)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs, outputs)\n\n\nclass CustomRNN(layers.Layer):\n    def __init__(self):\n        super(CustomRNN, self).__init__()\n        self.units = units\n        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n        # Our previously-defined Functional model\n        self.classifier = model\n\n    def call(self, inputs):\n        outputs = []\n        state = tf.zeros(shape=(inputs.shape[0], self.units))\n        for t in range(inputs.shape[1]):\n            x = inputs[:, t, :]\n            h = self.projection_1(x)\n            y = h + self.projection_2(state)\n            state = y\n            outputs.append(y)\n        features = tf.stack(outputs, axis=1)\n        print(features.shape)\n        return self.classifier(features)\n\n\nrnn_model = CustomRNN()\n_ = rnn_model(tf.zeros((1, timesteps, input_dim)))","metadata":{"id":"68s9xdcPfJxh","outputId":"a531112d-199d-4f0e-90ed-aa52db2ecffb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"units = 32\ntimesteps = 10\ninput_dim = 5\nbatch_size = 16\n\n\nclass CustomRNN(layers.Layer):\n    def __init__(self):\n        super(CustomRNN, self).__init__()\n        self.units = units\n        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n        self.classifier = layers.Dense(1)\n\n    def call(self, inputs):\n        outputs = []\n        state = tf.zeros(shape=(inputs.shape[0], self.units))\n        for t in range(inputs.shape[1]):\n            x = inputs[:, t, :]\n            h = self.projection_1(x)\n            y = h + self.projection_2(state)\n            state = y\n            outputs.append(y)\n        features = tf.stack(outputs, axis=1)\n        return self.classifier(features)\n\n\n# Note that you specify a static batch size for the inputs with the `batch_shape`\n# arg, because the inner computation of `CustomRNN` requires a static batch size\n# (when you create the `state` zeros tensor).\ninputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\nx = layers.Conv1D(32, 3)(inputs)\noutputs = CustomRNN()(x)\n\nmodel = keras.Model(inputs, outputs)\n\nrnn_model = CustomRNN()\n_ = rnn_model(tf.zeros((1, 10, 5)))","metadata":{"id":"UolIp7wFfLVv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## End to end example","metadata":{"id":"WmzbTQ2y03RY"}},{"cell_type":"code","source":"!git clone https://github.com/mlnuggets/tensorflow.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv tensorflow/data/labels-en.json labels-en.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv tensorflow/data/dataloader.py dataloader.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python dataloader.py -j labels-en.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_json(\"labels-en.json\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_names(externalId):\n    return f\"{externalId}.png\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"image_path\"] = df[\"externalId\"].map(image_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age = []\nhair = []\nbeard = []\nmustache = []\neye = []\ndef get_answers(tasks):\n    for all_tasks in tasks:\n        all_it = all_tasks[0]\n        for item in all_it['classifications']:\n                \n            if item['title'] == 'Age':\n                age.append(item['answer'])\n                \n            if item['title'] == 'Hair Color':\n                hair.append(item['answer'])\n                \n            if item['title'] == 'Beard Color':\n                beard.append(item['answer'])\n                \n            if item['title'] == 'Mustache Color':\n                mustache.append(item['answer'])\n                \n            if item['title'] == 'Eye Color':\n                eye.append(item['answer'])\n            ","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_answers(df['tasks'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['age'] = age","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hair_color'] = hair","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['beard_color'] = beard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['mustache_color'] = mustache","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['eye_color'] = eye","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['age'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngender_labelencoder = LabelEncoder()\nage_labelencoder = LabelEncoder()\nhair_labelencoder = LabelEncoder()\nbeard_labelencoder = LabelEncoder()\nmustache_labelencoder = LabelEncoder()\nglases_labelencoder = LabelEncoder()\neye_labelencoder = LabelEncoder()\n\ndf = df.assign(age = age_labelencoder.fit_transform(df[\"age\"]))\ndf = df.assign(hair_color = hair_labelencoder.fit_transform(df[\"hair_color\"]))\ndf = df.assign(beard_color = beard_labelencoder.fit_transform(df[\"beard_color\"]))\ndf = df.assign(mustache_color = mustache_labelencoder.fit_transform(df[\"mustache_color\"]))\ndf = df.assign(age = age_labelencoder.fit_transform(df[\"eye_color\"]))\ndf = df.assign(eye_color = eye_labelencoder.fit_transform(df[\"eye_color\"]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255, \n                                   shear_range=0.2,\n                                   zoom_range=0.2, \n                                   horizontal_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   validation_split=0.2\n                                   )\nvalidation_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = (128, 128)\nbatch_size = 32\nbase_dir = 'assets'\ntarget_columns = ['age',\n                  'hair_color','beard_color','mustache_color','eye_color',\n                 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(target_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntraining_set = train_datagen.flow_from_dataframe(df,base_dir,\n                                                seed=101,                                                 \n                                                target_size=image_size,\n                                                batch_size=batch_size,\n                                                x_col='image_path',\n                                                y_col=target_columns,\n                                                subset = 'training',\n                                                class_mode='multi_output')\nvalidation_set = validation_gen.flow_from_dataframe(df,base_dir, \n                                              target_size=image_size,\n                                              batch_size=batch_size, \n                                              x_col='image_path',\n                                              y_col=target_columns,\n                                              subset = 'validation',\n                                              class_mode='multi_output'\n                                                   )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in training_set:\n    for i in range(25):\n        ax = plt.subplot(5, 5, i + 1)\n        plt.imshow(images[i])\n        plt.axis(\"off\")\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training_set.labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x,y in training_set:\n#     print(x, y)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout,Resizing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n","metadata":{"id":"Y4Y9P-hzfOzN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_input = keras.Input(shape=(image_size[0], image_size[0], 3), name=\"images_input\") \n\nx = Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",name=\"first_block_conv2d\")(image_input)\nx = MaxPooling2D(pool_size=(2,2),name=\"first_block_maxpool2d\")(x)\nfirst_block_output = Flatten(name=\"first_block_flatten\")(x)\n\nx = Conv2D(filters=32,kernel_size=(3,3), activation='relu', name=\"second_block_conv2d\")(image_input)\nx = MaxPooling2D(pool_size=(2,2),name=\"second_block_maxpool2d\")(x)\nx = Flatten(name=\"second_block_flatten\")(x)\nsecond_block_output = layers.add([x, first_block_output], name=\"second_block_add\")\n\nx = Conv2D(filters=32,kernel_size=(3,3), activation='relu',name=\"third_block_conv2d\")(image_input)\nx = MaxPooling2D(pool_size=(2,2),name=\"third_block_maxpool2d\")(x)\nx = Flatten(name=\"third_block_flatten\")(x)\nthird_block_output = layers.add([x, second_block_output],name=\"third_block_add\")\n\n\nx = Dropout(0.25, name=\"dropout1\")(third_block_output)\nx = Dense(128, activation=\"relu\", name=\"dense1\")(x)\n\nage_prediction = Dense(df[\"age\"].nunique(), activation=\"softmax\",name=\"dense_age\")(x)\nhair_prediction = Dense(df[\"hair_color\"].nunique(), activation=\"softmax\",name=\"dense_hair\")(x)\nbeard_prediction = Dense(df[\"beard_color\"].nunique(), activation=\"softmax\",name=\"dense_beard\")(x)\nmustache_prediction = Dense(df[\"mustache_color\"].nunique(), activation=\"softmax\",name=\"dense_mustache\")(x)\neye_prediction = Dense(df[\"eye_color\"].nunique(), activation=\"softmax\",name=\"dense_eye\")(x)\n\nmodel = keras.Model(\n    inputs=image_input,\n    outputs=[\n             age_prediction,\n             hair_prediction,\n        beard_prediction,\n             mustache_prediction,\n        eye_prediction,\n    ],\n)\nkeras.utils.plot_model(model, \"model.png\",show_shapes=True)","metadata":{"id":"6fXrMsrG8o_J","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(keras.optimizers.RMSprop(), loss='sparse_categorical_crossentropy', metrics='sparse_categorical_accuracy')","metadata":{"id":"GtXcCXMO7eIT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(monitor='loss', patience=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=100\nhistory = model.fit(training_set,validation_data=validation_set, epochs=epochs, callbacks = [callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df = pd.DataFrame(history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(validation_set)","metadata":{"id":"OchWyObd7lwj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"loss\",\"val_loss\"]].plot();","metadata":{"id":"8UX_P2as7hn0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_age_loss\",\"val_dense_age_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_beard_loss\",\"val_dense_beard_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_hair_loss\",\"val_dense_hair_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_beard_loss\",\"val_dense_beard_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_mustache_loss\",\"val_dense_mustache_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_eye_loss\",\"val_dense_eye_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics_df[[\"dense_glasses_loss\",\"val_dense_glasses_loss\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_eye_sparse_categorical_accuracy\",\"val_dense_eye_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_eye_sparse_categorical_accuracy\",\"val_dense_eye_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_mustache_sparse_categorical_accuracy\",\"val_dense_mustache_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_beard_sparse_categorical_accuracy\",\"val_dense_beard_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_hair_sparse_categorical_accuracy\",\"val_dense_hair_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df[[\"dense_age_sparse_categorical_accuracy\",\"val_dense_age_sparse_categorical_accuracy\"]].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions","metadata":{}},{"cell_type":"code","source":"image_url = \"https://storage.googleapis.com/ango-covid-dataset/ffhq-dataset/batch2/25000.png\"\nimage_path = keras.utils.get_file('Sample_Food', origin=image_url)\ntest_image = keras.utils.load_img(\n    image_path, target_size=(image_size[0], image_size[1])\n)\nimport matplotlib.pyplot as plt\nplt.axis(\"off\")\nplt.imshow(test_image);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array = tf.keras.utils.img_to_array(test_image)\nimg_array = tf.expand_dims(img_array, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array = img_array / 255.0\npredictions = model.predict(img_array)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_predictions = predictions[0]\nhair_predictions = predictions[1]\nbeard_predictions = predictions[2]\nmustache_predictions = predictions[3]\neye_predictions = predictions[4]\nage_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"age_labelencoder.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_face_prediction(image_url):\n    import tensorflow as tf\n    import numpy as np\n    image_path = keras.utils.get_file('Face', origin=image_url)\n    test_image = keras.utils.load_img(\n    image_path, target_size=(image_size[0], image_size[1]))\n    img_array = tf.keras.utils.img_to_array(test_image)\n    img_array = tf.expand_dims(img_array, 0) \n    img_array = img_array / 255.0\n    predictions = model.predict(img_array)\n    age_predictions = predictions[0][0]\n    hair_predictions = predictions[1][0]\n    beard_predictions = predictions[2][0]\n    mustache_predictions = predictions[3][0]\n    eye_predictions = predictions[4][0]\n        \n    age_scores = tf.nn.softmax(age_predictions).numpy()\n    \n    hair_scores = tf.nn.softmax(hair_predictions).numpy()\n    \n    beard_scores = tf.nn.softmax(beard_predictions).numpy()\n    \n    mustache_scores = tf.nn.softmax(mustache_predictions).numpy()\n    \n    eye_scores = tf.nn.softmax(eye_predictions).numpy()\n    \n    \n    print(f\"Age: {list(age_labelencoder.classes_)[np.argmax(age_scores)]} with a { (100 * np.max(age_scores)).round(2) } percent confidence.\")\n    print(f\"Hair Color: {list(hair_labelencoder.classes_)[np.argmax(hair_scores)]} with a { (100 * np.max(hair_scores)).round(2) } percent confidence.\") \n    print(f\"Beard Color: {list(beard_labelencoder.classes_)[np.argmax(beard_scores)]} with a { (100 * np.max(beard_scores)).round(2) } percent confidence.\") \n    print(f\"Mustache Color: {list(mustache_labelencoder.classes_)[np.argmax(mustache_scores)]} with a { (100 * np.max(mustache_scores)).round(2) } percent confidence.\") \n    print(f\"Eye Color: {list(eye_labelencoder.classes_)[np.argmax(eye_scores)]} with a { (100 * np.max(eye_scores)).round(2) } percent confidence.\") \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_face_prediction('https://storage.googleapis.com/ango-covid-dataset/ffhq-dataset/batch2/25000.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Appendix: How to add layers","metadata":{}},{"cell_type":"code","source":"input_shape = (2, 3, 4)\nx1 = tf.random.normal(input_shape)\nx2 = tf.random.normal(input_shape)\ny = tf.keras.layers.Add()([x1, x2])\nprint(y.shape)","metadata":{"id":"HZjIm6y6Bm4W","outputId":"356b61e6-d20b-428e-e782-52d2f7d8b735","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1","metadata":{"id":"zU5loFr1BsRJ","outputId":"1c285b3b-93eb-4957-9a01-f2efc31d4134","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2","metadata":{"id":"qrRuhWvMBtwZ","outputId":"f14b06ec-91a6-49a6-d714-41ccc0575681","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"id":"OLd8dvAPBufQ","outputId":"c772c685-dcd2-4dc2-d9ca-8a58c5e03095","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1.1024796 +  0.02846527","metadata":{"id":"t1FD2yWOBw4p","outputId":"9249c760-f5bb-49db-cb5b-ac998c98fa39","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Where to go from here\nFollow us on [LinkedIn](https://www.linkedin.com/company/mlnuggets), [Twitter](https://twitter.com/ml_nuggets), [GitHub](https://github.com/mlnuggets) and subscribe to our [blog](https://www.machinelearningnuggets.com/#/portal) so that you don't miss a new issue.","metadata":{"id":"RxU9UNCFB8BD"}}]}