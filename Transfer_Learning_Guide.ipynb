{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning guide(With examples for text and images in Keras and PyTorch)\n"
      ],
      "metadata": {
        "id": "JisorUyUaeFV"
      },
      "id": "JisorUyUaeFV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Click the image below to read the post online.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://www.machinelearningnuggets.com/transfer-learning-guide\"><img src=\"https://digitalpress.fra1.cdn.digitaloceanspaces.com/mhujhsj/2022/07/logho-1.png\" alt=\"Open in ML Nuggets\"></a>"
      ],
      "metadata": {
        "id": "jA0-dW3lZ6lM"
      },
      "id": "jA0-dW3lZ6lM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67fe1606"
      },
      "source": [
        "#### Keras"
      ],
      "id": "67fe1606"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f533b1da",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "IMAGE_SIZE = 224 # define images size\n",
        "pretrained_model = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "    alpha=1.0,\n",
        "    include_top=True,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation=\"softmax\"\n",
        ")\n",
        "#\n",
        "pretrained_model.trainable = False\n",
        "#summary of the architecture\n",
        "#pretrained_model.summary()"
      ],
      "id": "f533b1da"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffefef4b"
      },
      "source": [
        "#### TensorFlow Hub"
      ],
      "id": "ffefef4b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "458f46b5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "#link to the pre-trained model\n",
        "mobilenet_v2 =\"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\"\n",
        "#define the model name you want to acquire\n",
        "classifier_model = mobilenet_v2\n",
        "\n",
        "IMAGE_SHAPE = 224\n",
        "\n",
        "classifier = tf.keras.Sequential([\n",
        "    hub.KerasLayer(classifier_model, input_shape=(IMAGE_SHAPE, IMAGE_SHAPE, 3))\n",
        "])\n",
        "classifier.summary()"
      ],
      "id": "458f46b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "471f2d16"
      },
      "outputs": [],
      "source": [
        "classifier.summary()"
      ],
      "id": "471f2d16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec70875f"
      },
      "source": [
        "#### Word Embeddings"
      ],
      "id": "ec70875f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvcTOBuPxdPS"
      },
      "source": [
        "##### Glove"
      ],
      "id": "jvcTOBuPxdPS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aea6ec3a"
      },
      "outputs": [],
      "source": [
        "# download glove and unzip it in Notebook.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "id": "aea6ec3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPzamQ58swXK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "  \n",
        "x = {'the', 'match', 'score', 'prime',\n",
        "     'player', 'manager', 'league'}\n",
        "  \n",
        "# create the dict.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "  \n",
        "# number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\", \n",
        "      len(tokenizer.word_index))\n",
        "print(\"Dictionary is = \", tokenizer.word_index)\n",
        "def embedding_for_vocab(filepath, word_index,\n",
        "                        embedding_dim):\n",
        "    vocab_size = len(word_index) + 1\n",
        "      \n",
        "    # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
        "                                       embedding_dim))\n",
        "  \n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix_vocab[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "  \n",
        "    return embedding_matrix_vocab\n",
        "  \n",
        "  \n",
        "# matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "    'glove.6B.50d.txt', tokenizer.word_index,\n",
        "  embedding_dim)\n",
        "  \n",
        "print(\"Dense vector for first entry is => \",\n",
        "      embedding_matrix_vocab[1])"
      ],
      "id": "UPzamQ58swXK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROL48JF7xa4a"
      },
      "source": [
        "##### Word2Vec"
      ],
      "id": "ROL48JF7xa4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZsbd3ABzlyV"
      },
      "outputs": [],
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "id": "cZsbd3ABzlyV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHTDO7-S1PIu"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ],
      "id": "lHTDO7-S1PIu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98k3QgTGxXVP"
      },
      "outputs": [],
      "source": [
        "!wget http://vectors.nlpl.eu/repository/20/51.zip\n"
      ],
      "id": "98k3QgTGxXVP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsDMEt8n6TGM"
      },
      "outputs": [],
      "source": [
        "#unzip\n",
        "!unzip 51.zip"
      ],
      "id": "nsDMEt8n6TGM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L70nIBza67th"
      },
      "outputs": [],
      "source": [
        "#download the model\n",
        "!wget http://vectors.nlpl.eu/repository/20/51.zip\n",
        "#unzip\n",
        "!unzip 51.zip\n",
        "#gzip the model for loading\n",
        "!gzip model.bin"
      ],
      "id": "L70nIBza67th"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptGfRafV6H_G"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "EMBEDDING_FILE = 'model.bin.gz'\n",
        "word_vectors = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "#get most similar words in the word vector\n",
        "result = word_vectors.most_similar(positive=['player', 'league'], negative=['man'])\n",
        "most_similar_key, similarity = result[0]  # look at the first match\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ],
      "id": "ptGfRafV6H_G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8iZTYwc7Llg"
      },
      "outputs": [],
      "source": [
        "result = word_vectors.most_similar(positive=['player', 'league'], negative=['man'])\n",
        "most_similar_key, similarity = result[0]  # look at the first match\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")"
      ],
      "id": "X8iZTYwc7Llg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHqPu70F9O-v"
      },
      "source": [
        "##### FastText"
      ],
      "id": "fHqPu70F9O-v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHGwjbjD9iyA"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n"
      ],
      "id": "rHGwjbjD9iyA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCsGr49__ycL"
      },
      "outputs": [],
      "source": [
        "!pip install gluonnlp\n",
        "#mxnet\n",
        "!pip install mxnet"
      ],
      "id": "vCsGr49__ycL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4bMq4MO9S5s"
      },
      "outputs": [],
      "source": [
        "import gluonnlp as nlp\n",
        "#create a word embedding instance by calling nlp.embedding.create\n",
        "fasttext_model = nlp.embedding.create('fasttext', source='wiki.simple')"
      ],
      "id": "U4bMq4MO9S5s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZcy3-LcBSrY"
      },
      "outputs": [],
      "source": [
        "def tokenizer(source_str, token_delim=' ', seq_delim='\\n'):\n",
        "    import re\n",
        "    '''Utility function for tokenizing'''\n",
        "    tokens = filter(None, re.split(token_delim + '|' + seq_delim, source_str))\n",
        "    return tokens\n",
        "sentence = \"The player scored twice during the match\"\n",
        "counter = nlp.data.count_tokens(tokenizer(sentence))\n",
        "#create vocabulary\n",
        "vocab = nlp.Vocab(counter)\n",
        "#attach embedding\n",
        "vocab.set_embedding(fasttext_model)\n",
        "#check the embedding vector\n",
        "\n",
        "vocab.embedding['player'][:5]"
      ],
      "id": "sZcy3-LcBSrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvR5VyIDxw6"
      },
      "source": [
        "#### Hugging Face"
      ],
      "id": "gjvR5VyIDxw6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QTlE3MkD00X"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece\n"
      ],
      "id": "_QTlE3MkD00X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrL7PC2WGB-4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "sentence = \"The player scored twice during the match in Moscow and helped Brendan Rodgers manager win the league\"\n",
        "\n",
        "ner_results = nlp(sentence)\n",
        "print(ner_results)\n"
      ],
      "id": "qrL7PC2WGB-4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "444420f4"
      },
      "source": [
        "#### PyTorch"
      ],
      "id": "444420f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f97b429"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "model_conv = torchvision.models.resnet18(pretrained=True) "
      ],
      "id": "6f97b429"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feOsqL4nZ2m2"
      },
      "source": [
        "#### Prediction"
      ],
      "id": "feOsqL4nZ2m2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7523032"
      },
      "outputs": [],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16()\n",
        "print(model.summary())"
      ],
      "id": "e7523032"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM0YhhpH5T-V"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "image_url = \"https://unsplash.com/photos/u_kMWN-BWyU/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjYyODMwNjAy&force=true\"\n",
        "img_data = requests.get(image_url).content\n",
        "with open('satyabratasm-u_kMWN-BWyU-unsplash.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)"
      ],
      "id": "cM0YhhpH5T-V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7576b20"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "# load an image from path\n",
        "path = 'satyabratasm-u_kMWN-BWyU-unsplash.jpg'\n",
        "img = load_img(path, target_size=(224, 224))\n",
        "from keras.preprocessing.image import img_to_array\n",
        "# convert the  pixels to a numpy array\n",
        "img = img_to_array(img)\n",
        "\n",
        "# reshape data for the pre-trained VGG model\n",
        "img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
        "\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "# transform the img for the pre-trained VGG model\n",
        "img = preprocess_input(img)\n",
        "# predict the probability for the output classes used in ImageNet\n",
        "yhat = model.predict(img)\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "# convert the probabilities to discrete class labels\n",
        "label = decode_predictions(yhat, top = 5)\n",
        "# Get the most likely output with the highest probability\n",
        "label = label[0][0]\n",
        "# Show the predicted class\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "id": "f7576b20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "160fe9de"
      },
      "outputs": [],
      "source": [],
      "id": "160fe9de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a19a84d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "image_path = 'satyabratasm-u_kMWN-BWyU-unsplash.jpg'\n",
        "image = tf.keras.utils.load_img(image_path, target_size=(224, 224))\n",
        "from keras.preprocessing.image import img_to_array\n",
        "image_data = img_to_array(image)\n",
        "\n",
        "image_data = np.expand_dims(image_data, axis=0)\n",
        "image_data = preprocess_input(image_data)\n",
        "\n",
        "extracted_features = model.predict(image_data)\n",
        "\n",
        "print (extracted_features.shape)"
      ],
      "id": "0a19a84d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR6ULDU_Mi-z"
      },
      "source": [
        "#### Fine-Tuning"
      ],
      "id": "VR6ULDU_Mi-z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZWb8vOHMvg3"
      },
      "outputs": [],
      "source": [
        "# begin by unfreezing all layers of the base model\n",
        "model.trainable = True\n",
        "\n",
        "#Apart from the 5 last layers, freeze all the other layers\n",
        "for layer in model.layers[:-5]: \n",
        "    layer.trainable = False\n",
        "\n",
        "# compile and retrain with a very low learning rate\n",
        "# compile and start training after freezing the layers\n",
        "learning_rate = 1e-4\n",
        "low_learning_rate = learning_rate / 100\n",
        "#recompile the model with the new learning rate\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.RMSprop(learning_rate = low_learning_rate), \n",
        "              metrics = ['acc']\n",
        ")\n"
      ],
      "id": "gZWb8vOHMvg3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9YdGurHtNbV"
      },
      "source": [
        "#### Transfer learning with image data"
      ],
      "id": "Z9YdGurHtNbV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp_dZ4IPMv8f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "Kp_dZ4IPMv8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYoonZSKtZ97"
      },
      "source": [
        "##### Getting the Dataset"
      ],
      "id": "JYoonZSKtZ97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDQGYE9ztSep"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#tfds.disable_progress_bar()\n",
        "\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 20% for validation and 10% for test\n",
        "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "print(\"There are %d training samples\" % tf.data.experimental.cardinality(train_data))\n",
        "print(\n",
        "    \"There are %d validation samples\" % tf.data.experimental.cardinality(validation_data)\n",
        ")\n",
        "print(\"There are %d test samples\" % tf.data.experimental.cardinality(test_data))"
      ],
      "id": "hDQGYE9ztSep"
    },
    {
      "cell_type": "code",
      "source": [
        "#if the link below is broken, go to https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "#to obtain a new download link\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n",
        "#remove previous files\n",
        "!rm -rf PetImages\n",
        "#unzip the dataset\n",
        "!unzip -qq kagglecatsanddogs_5340.zip"
      ],
      "metadata": {
        "id": "BUAYel4G3_Oo"
      },
      "id": "BUAYel4G3_Oo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVs0vmQHt9-J"
      },
      "source": [
        "##### Load dataset"
      ],
      "id": "uVs0vmQHt9-J"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "dir = \"PetImages/\"\n",
        "data = image_dataset_from_directory(dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=32,\n",
        "                                             image_size=(150, 150))"
      ],
      "metadata": {
        "id": "ReY_bT3O9YKq"
      },
      "id": "ReY_bT3O9YKq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HQXd7NwQ9ex8"
      },
      "id": "HQXd7NwQ9ex8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRbYx_1etSm2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i, (img, label) in enumerate(train_data.take(4)):\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample images (Cat :0, Dog:1)\")\n",
        "plt.show()"
      ],
      "id": "wRbYx_1etSm2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLU8SwEbvFMh"
      },
      "source": [
        "##### Data Preprocessing"
      ],
      "id": "CLU8SwEbvFMh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmrPWC1BtSyF"
      },
      "outputs": [],
      "source": [
        "size = (150, 150)\n",
        "\n",
        "train_data = train_data.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "validation_data = validation_data.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "test_data = test_data.map(lambda x, y: (tf.image.resize(x, size), y))"
      ],
      "id": "hmrPWC1BtSyF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHPLMpJ7vT8V"
      },
      "source": [
        "Batch the data and cache to prevent loading the dataset each time we need it to optimize loading speeds"
      ],
      "id": "rHPLMpJ7vT8V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZVmokaiMwYv"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_data = train_data.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "validation_data = validation_data.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_data = test_data.cache().batch(batch_size).prefetch(buffer_size=10)"
      ],
      "id": "CZVmokaiMwYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XRyDwRGvqC4"
      },
      "source": [
        "We have a small dataset, therefore, it is advisable to initiate sample diversity by applying random but realistic transformations to the training data. , Some of the transformations for image data include:\n",
        "1. Random horizontal flipping or small random rotations.\n",
        "2. Gray-scaling\n",
        "3. Shifts\n",
        "4. Flips\n",
        "5. Brightness\n",
        "6. Zoom\n",
        "\n",
        "Data augmentation helps to expose the model to different aspects of the training data which helps to prevent overfitting."
      ],
      "id": "-XRyDwRGvqC4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_GOisJew_Nl"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.RandomFlip(\"horizontal\"), #flips images \n",
        "     layers.RandomRotation(0.1),#randomly rotates images\n",
        "     #layers.RandomZoom(.5, .2), #randomly zooms images \n",
        "     layers.RandomFlip(\n",
        "    mode=\"horizontal_and_vertical\", seed=None #randomly flips images\n",
        ")\n",
        "]\n",
        ")"
      ],
      "id": "M_GOisJew_Nl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Rgh0XdyOPT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for images, labels in train_data.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[7]\n",
        "    for i in range(4):\n",
        "        ax = plt.subplot(2, 2, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample preprocessed image\")\n",
        "plt.show();"
      ],
      "id": "a2Rgh0XdyOPT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-PL4FwR0KFM"
      },
      "source": [
        "##### Create a base model from the pre-trained Inception model"
      ],
      "id": "W-PL4FwR0KFM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzwtft3YyOXy"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.InceptionV3(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,  #Exclude ImageNet classifier at the top\n",
        ")\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False"
      ],
      "id": "uzwtft3YyOXy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qZbOb6q1g2t"
      },
      "source": [
        "##### Create the final dense layer"
      ],
      "id": "4qZbOb6q1g2t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6oUPxlmyOfO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create new model on top\n",
        "#standardize the input\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Inception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "#rescale\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "id": "q6oUPxlmyOfO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train the model"
      ],
      "metadata": {
        "id": "5eHLxR1UCLrv"
      },
      "id": "5eHLxR1UCLrv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj-BD1BtyOtH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "!rm -rf image_logs\n",
        "%load_ext tensorboard\n",
        "log_folder = 'image_logs'\n",
        "callbacks = [\n",
        "            EarlyStopping(patience = 3),\n",
        "            TensorBoard(log_dir=log_folder)\n",
        "            ]\n",
        "\n",
        "#compile the model to \n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=keras.metrics.BinaryAccuracy())\n",
        "hist = model.fit(train_data,\n",
        "                 epochs=5, \n",
        "                 validation_data = validation_data, callbacks = callbacks)"
      ],
      "id": "Wj-BD1BtyOtH"
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate performance on test data\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(\"Model accuracy:\", round(accuracy, 4)*100)\n",
        "print(\"Model loss:\", round(loss, 4))"
      ],
      "metadata": {
        "id": "PronWHovaVgh"
      },
      "id": "PronWHovaVgh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {'image_logs/'}"
      ],
      "metadata": {
        "id": "jZFdz9CDM54B"
      },
      "id": "jZFdz9CDM54B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zQG9OfCw1K"
      },
      "source": [
        "#### Fine-tuning the model"
      ],
      "id": "T5zQG9OfCw1K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjyLEpPLC1QC"
      },
      "outputs": [],
      "source": [
        "#unfreeze the base model\n",
        "base_model.trainable = False\n",
        "#Apart from the 10 last layers, freeze all the other layers\n",
        "for layer in model.layers[:-10]: \n",
        "    layer.trainable = True\n",
        "model.summary()\n",
        "#define the learning rate\n",
        "learning_rate = 1e-5\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),  # Low learning rate\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")"
      ],
      "id": "CjyLEpPLC1QC"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "!rm -rf fine_tune_logs\n",
        "%load_ext tensorboard\n",
        "log_folder = 'fine_tune_logs'\n",
        "callbacks = [\n",
        "            EarlyStopping(patience = 5),\n",
        "            TensorBoard(log_dir=log_folder)\n",
        "            ]\n",
        "epochs = 5\n",
        "hist1 = model.fit(train_data,\n",
        "          epochs=epochs,\n",
        "          validation_data=validation_data,callbacks=callbacks)"
      ],
      "metadata": {
        "id": "xnPklPwzLXQq"
      },
      "id": "xnPklPwzLXQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d600tWfaC1gH"
      },
      "outputs": [],
      "source": [
        "#evaluate performance on test data\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(\"Fine-tuned model accuracy:\", round(accuracy, 4)*100)\n",
        "print(\"Fine-tuned model loss:\", round(loss, 4))"
      ],
      "id": "d600tWfaC1gH"
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {'fine_tune_logs/'}"
      ],
      "metadata": {
        "id": "kVnbex9PNzl-"
      },
      "id": "kVnbex9PNzl-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC2R7pWVdqEX"
      },
      "source": [
        "#### Example of transfer learning with natural language processing"
      ],
      "id": "CC2R7pWVdqEX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MPRIZAQgKIp"
      },
      "source": [
        "#### Pretrained word embeddings"
      ],
      "id": "6MPRIZAQgKIp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KapIg_Nhp4Zr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, SpatialDropout1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "id": "KapIg_Nhp4Zr"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
        "!unzip drugsCom_raw.zip"
      ],
      "metadata": {
        "id": "8LyEYmnJjKg5"
      },
      "id": "8LyEYmnJjKg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4NlKjMYl_qz"
      },
      "outputs": [],
      "source": [
        "#read the data\n",
        "df = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t')\n",
        "#create sentiment column\n",
        "df['category'] = [1 if int(x)>5 else 0 for x in df['rating']]\n",
        "#get relevant variables\n",
        "df = df[['review', 'category']].copy()\n",
        "df.head()"
      ],
      "id": "A4NlKjMYl_qz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Pre-processing"
      ],
      "metadata": {
        "id": "IjDPlRytdrcn"
      },
      "id": "IjDPlRytdrcn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Tokenizing the words"
      ],
      "metadata": {
        "id": "9tFhymLrduTQ"
      },
      "id": "9tFhymLrduTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "max_features = 10000  # Maximum vocabulary size.\n",
        "max_len = 100 # Sequence length to pad the outputs to.\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',max_tokens=max_features,output_mode='int',output_sequence_length=max_len)\n",
        "vectorize_layer.adapt(list((df['review'].values)),batch_size=None)"
      ],
      "metadata": {
        "id": "jgDlcVfMLy9c"
      },
      "id": "jgDlcVfMLy9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_t = list((df['review'].values))\n",
        "y = to_categorical(df['category'])\n",
        "X_train, X_test , y_train, y_test = train_test_split(X_t, y , test_size = 0.30)\n",
        "#apply cetorization layer to train and test\n",
        "X_train =  vectorize_layer(X_train)\n",
        "X_test =  vectorize_layer(X_test)"
      ],
      "metadata": {
        "id": "prZp4yY8d_O2"
      },
      "id": "prZp4yY8d_O2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['words in sentence'] = [len(item.split()) for item in df.review]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H-5zyy7ir_1F"
      },
      "id": "H-5zyy7ir_1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using GloVe Embeddings"
      ],
      "metadata": {
        "id": "KWgnfnQRe1bM"
      },
      "id": "KWgnfnQRe1bM"
    },
    {
      "cell_type": "code",
      "source": [
        "#download glove embeddings\n",
        "# download glove and unzip it in Notebook.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "C35adWHxe3Fe"
      },
      "id": "C35adWHxe3Fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load your embeddings\n",
        "embeddings_index = {}\n",
        "emb = open('glove.6B.100d.txt')\n",
        "for sentence in emb:\n",
        "    values = sentence.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "emb.close()\n",
        "\n",
        "print('There are %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "ExNp_tajfwKY"
      },
      "id": "ExNp_tajfwKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get vocabulary\n",
        "voc = vectorize_layer.get_vocabulary()\n",
        "#create a word index\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "9S-dLQHj9PJw"
      },
      "id": "9S-dLQHj9PJw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index"
      ],
      "metadata": {
        "id": "eCbFAtRHYhvQ"
      },
      "id": "eCbFAtRHYhvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create embedding matrix"
      ],
      "metadata": {
        "id": "OQUkYMd_9mgi"
      },
      "id": "OQUkYMd_9mgi"
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1"
      ],
      "metadata": {
        "id": "-cTy0IkFAlqI"
      },
      "id": "-cTy0IkFAlqI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "jw64LL3Icne2"
      },
      "id": "jw64LL3Icne2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create the embedding layer"
      ],
      "metadata": {
        "id": "sd3VJFMfgbsQ"
      },
      "id": "sd3VJFMfgbsQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow import keras\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim = num_tokens,\n",
        "    output_dim = embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "TNICJVWlgUk6"
      },
      "id": "TNICJVWlgUk6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the model"
      ],
      "metadata": {
        "id": "AmUbADTzhSQE"
      },
      "id": "AmUbADTzhSQE"
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "from tensorflow.keras.layers import Flatten\n",
        "model = Sequential()\n",
        "vocab_size = 10002\n",
        "#use the embedding_matrix\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Bidirectional(LSTM(10, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "pBauJF6A3PP8"
      },
      "id": "pBauJF6A3PP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
      ],
      "metadata": {
        "id": "604G1Mjsifo7"
      },
      "id": "604G1Mjsifo7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYukLrjptyN3",
        "outputId": "7c3bff28-67b0-4132-e3c2-799efdadec0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/45 [=====>........................] - ETA: 31s - loss: 0.5131 - accuracy: 0.7562"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf embed_logs\n",
        "log_folder = 'embed_logs'\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "#apply callbacks\n",
        "callbacks = [\n",
        "            EarlyStopping(patience = 3),\n",
        "            TensorBoard(log_dir=log_folder)\n",
        "            ]\n",
        "#compile\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "num_epochs = 10\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test),callbacks=callbacks, batch_size = 2560)"
      ],
      "id": "pYukLrjptyN3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STDrjq_TuJs0"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(X_test,y_test)\n",
        "print('Test accuracy :', round(accuracy, 4))\n",
        "print(\"Test Loss:\", round(loss, 4))"
      ],
      "id": "STDrjq_TuJs0"
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {'embed_logs/'}"
      ],
      "metadata": {
        "id": "eZ33RkwGqG1-"
      },
      "id": "eZ33RkwGqG1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to go from here\n",
        "Follow us on [LinkedIn](https://www.linkedin.com/company/mlnuggets), [Twitter](https://twitter.com/ml_nuggets), [GitHub](https://github.com/mlnuggets) and subscribe to our [blog](https://www.machinelearningnuggets.com/#/portal) so that you don't miss a new issue."
      ],
      "metadata": {
        "id": "0mGQyoFeaC2h"
      },
      "id": "0mGQyoFeaC2h"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}